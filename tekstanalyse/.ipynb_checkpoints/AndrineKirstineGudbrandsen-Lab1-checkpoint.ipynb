{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Managing text and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"further\", \"Forward\", \"Foreign\", \"financE\", \"Forgive\", \"feature\", \"federal\",\n",
    "\"failurE\", \"Feeling\", \"finding\", \"freedom\", \"Foundry\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Forward', 'Foreign', 'Forgive', 'Foundry']\n"
     ]
    }
   ],
   "source": [
    "fo = filter(lambda w: w.lower().startswith(\"fo\"), words)\n",
    "\n",
    "print(list(fo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['financE', 'Forgive', 'feature', 'failurE']\n"
     ]
    }
   ],
   "source": [
    "e = filter(lambda w: w.lower().endswith(\"e\"), words)\n",
    "\n",
    "print(list(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple technics like this can bring value to a project by making us think easy when solving uncomplicasted problems like filterings a list of words. A more complex solution will just add a lot of times and often a easier solution will be prefered in project where other will read and use your code and makes it all more self-explanatory. A problem is often just as complex as it is made into. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the issues by converting words to lowercase to text clean may be that it can loose some important data formatting, it may also be an expensive prossess if we have to deal with a big dataset. An other issue may be that it is very time-consuming and if it is done by a automated prossess it may mishandle the data and corrupt it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - reading a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "#print(brown.categories())\n",
    "\n",
    "cat1 = brown.words(categories=\"adventure\")\n",
    "cat2 = brown.words(categories=\"humor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3780, 'and': 1706, 'a': 1432, 'of': 1327, 'to': 1322, 'he': 1283, '``': 998, \"''\": 995, 'was': 919, 'in': 892, ...})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = nltk.FreqDist([w.lower() for w in cat1 if w not in string.punctuation])\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 1027, 'and': 529, 'a': 523, 'of': 518, 'to': 465, 'in': 360, '``': 343, \"''\": 340, 'was': 276, 'that': 252, ...})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2 = nltk.FreqDist([w.lower() for w in cat2 if w not in string.punctuation])\n",
    "list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in category 1 the most used words are: 'the', 'and', 'a', 'of', and 'to'.\n",
    "And in category 2 the most used words are: 'the', 'of', 'and', 'a', and 'to'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we got the same five words as the most used in both categories. These words are commonly used in almost every sentence and will therefor often be the most used, wheater or not it gives any iformation. By looking at the most used words in these categeories, we can not make any assumtions of what it is about. These words are what is often refered to as stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 4057, ',': 3488, '``': 998, \"''\": 995, '?': 518, '!': 314, 'said': 288, ';': 216, '--': 213, 'would': 194, ...})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = nltk.FreqDist([w.lower() for w in cat1 if w.lower() not in stopwords])\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 1331, '.': 877, '``': 343, \"''\": 340, '?': 152, ';': 98, 'said': 88, '!': 82, 'one': 77, '--': 67, ...})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2 = nltk.FreqDist([w.lower() for w in cat2 if w.lower() not in stopwords])\n",
    "list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 most frequent words are 'would', 'one', 'back', 'man', and 'could' for the first category.\n",
    "And 'said', 'one', 'would', 'time', and 'even' for the category 2. \n",
    "\n",
    "It seems like for the first category, adventure, the words represent intention, while for the humour category they indicate dialog. Here we can see clear difference in what the most used words describe and acatually understand somewhat the plot or point of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the techniques is to remove punctuations to make the word list easier to look at to find what we actually want.\n",
    "The secund one is to look at the context in where the words are used. Especially in humor, the context is importante to make it funny and therefor relevant to understand the content of the texts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'said': 288, 'would': 194, 'one': 183, 'back': 166, 'man': 166, 'could': 154, 'like': 141, 'time': 129, 'get': 101, 'eyes': 98, ...})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = nltk.FreqDist([w.lower() for w in cat1 if w.lower() not in stopwords and w.isalnum()])\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'said': 88, 'one': 77, 'would': 56, 'time': 44, 'even': 39, 'like': 36, 'could': 33, 'way': 28, 'things': 28, 'two': 26, ...})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2 = nltk.FreqDist([w.lower() for w in cat2 if w.lower() not in stopwords and w.isalnum()])\n",
    "list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can easier see more words and get a clearer difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConcatenatedCorpusView' object has no attribute 'plt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-d7ff0818ab00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispersion_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"said\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dumb\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConcatenatedCorpusView' object has no attribute 'plt'"
     ]
    }
   ],
   "source": [
    "cat1.plt.dispersion_plot([\"said\", \"dumb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Building your own corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-7ce8bde46a39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwebdriver_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchrome\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get('https://www.sb.no/')\n",
    "content = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(content, features=\"html.parser\")\n",
    "\n",
    "tabs = soup.find_all('article', attrs={\"class\": \"story\"})\n",
    "\n",
    "for tab in tabs:\n",
    "    if (len(tab.select('a.story-text')) >= 1):\n",
    "        text = tab.select('a.story-text')[0].get_text()\n",
    "        print(text)\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
