{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following can be of use when identifying tags:\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the familiar brown corpus to begin with. Get the POS-tagged sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "untagged_sents = nltk.corpus.brown.sents()\n",
    "sents = []\n",
    "for sent in untagged_sents:\n",
    "    sents.append(nltk.pos_tag(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 1 - Introduction to chunking\n",
    "\n",
    "## 1a)\n",
    "Make your own noun phrase (NP) chunker, detecting noun phrases and a clause, for which verbs (VB) are followed by a preposition (IN) and/or a noun phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "NP: {<DT><JJ>?<NN.*>+} # Determinant -> optional adjective -> noun(s)\n",
    "CLAUSE: {<VB.*><IN|NP>} # Verb -> preposition and/or noun-phrase\n",
    "\"\"\"\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(grammar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,560.0,168.0\" width=\"560px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"7.14286%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">He</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.57143%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.14286%\" x=\"7.14286%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.7143%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.14286%\" x=\"14.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">not</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"17.1429%\" x=\"21.4286%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">interested</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.71429%\" x=\"38.5714%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.4286%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10%\" x=\"44.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">being</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10%\" x=\"54.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">named</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35.7143%\" x=\"64.2857%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP2</text></svg><svg width=\"16%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"44%\" x=\"16%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">full-time</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"40%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">director</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.1429%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('He', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('interested', 'JJ'), ('in', 'IN'), ('being', 'VBG'), ('named', 'VBN'), Tree('NP2', [('a', 'DT'), ('full-time', 'JJ'), ('director', 'NN')])])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your parser!x\n",
    "test_sentence = sents[400][:10] # just an example sentence, using the first 10 words\n",
    "chunks = chunk_parser.parse(test_sentence)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Tree('S', [('He', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('interested', 'JJ'), ('in', 'IN'), ('being', 'VBG'), Tree('CLAUSE', [('named', 'VBN'), Tree('NP', [('a', 'DT'), ('full-time', 'JJ'), ('director', 'NN')])])])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b)\n",
    "Convert a POS tagged text to a list of tuples, where each tuple consists of a verb followed by a sequence of noun phrases and prepositions.\n",
    "Example: “the little cat sat on the mat” becomes (‘sat’, ‘on’, ‘NP’) . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_verb_NP_tuples(tagged_sents):\n",
    "    tuples = set()\n",
    "    for sent in tagged_sents:\n",
    "        tree = chunk_parser.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"CLAUSE\":\n",
    "                tuples.add(\" \".join([w for w, POS in subtree.leaves()]))\n",
    "    return list(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['try for',\n",
       " 'provide a major market',\n",
       " 'detect any obvious mistakes',\n",
       " 'have no redeeming',\n",
       " 'slapped the reins',\n",
       " 'had the proof',\n",
       " 'experienced among',\n",
       " 'living from',\n",
       " 'exhibited by',\n",
       " 'control than',\n",
       " 'leave the matter',\n",
       " 'have the car',\n",
       " 'having a special interest',\n",
       " 'are no exception',\n",
       " 'has a carnival',\n",
       " 'employed by',\n",
       " 'watch the baby',\n",
       " 'suppose the same emotion',\n",
       " 'mistrusted the whole lot',\n",
       " 'chose a white brocade gown']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your output :-)\n",
    "import random\n",
    "\n",
    "vb_np = chunks_to_verb_NP_tuples(sents)\n",
    "random.shuffle(vb_np)\n",
    "vb_np[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```['try for',\n",
    " 'provide a major market',\n",
    " 'detect any obvious mistakes',\n",
    " 'have no redeeming',\n",
    " 'slapped the reins',\n",
    " 'had the proof',\n",
    " 'experienced among',\n",
    " 'living from',\n",
    " 'exhibited by',\n",
    " 'control than',\n",
    " 'leave the matter',\n",
    " 'have the car',\n",
    " 'having a special interest',\n",
    " 'are no exception',\n",
    " 'has a carnival',\n",
    " 'employed by',\n",
    " 'watch the baby',\n",
    " 'suppose the same emotion',\n",
    " 'mistrusted the whole lot',\n",
    " 'chose a white brocade gown']```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c)\n",
    "Using the pre-annotated test set from wall street journal data (conll2000 in nltk), experiment with different grammars to get the highest possible F-measure. There is no evaluation of this task, but rather a motivator to learn something about grammars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RegexpParser' object has no attribute 'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-db546bcefe84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mchunk_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegexpParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"NP: {<[CDJNP].*>+}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#TOdone: your parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RegexpParser' object has no attribute 'accuracy'"
     ]
    }
   ],
   "source": [
    "wsj = nltk.corpus.conll2000\n",
    "test_sents = wsj.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(r\"NP: {<[CDJNP].*>+}\") #TOdone: your parser\n",
    "print(chunk_parser.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 2 - Making use of chunks\n",
    "\n",
    "## 2a)\n",
    "With the following grammar rules:\n",
    "```\n",
    "1. proper noun singular\n",
    "2. determiner followed by an adjective, followed by any noun\n",
    "3. two consecutive nouns\n",
    "```\n",
    "Create a `RegexpParser` chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "NP1: {<NNP>} # Noun, proper, singular\n",
    "NP2: {<DT><JJ.*><NN.*>} # Determiner -> adjective (incl. comparative and superlative) -> any noun\n",
    "NP3: {<NN.*><NN.*>} # Noun (any) -> noun (any)\n",
    "\"\"\"\n",
    "# TOdone\n",
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b)\n",
    "\n",
    "Read the file `starlink.txt` and perform the following operations on the text:\n",
    "- sentence tokenize\n",
    "- word tokenize\n",
    "- pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1656.0,168.0\" width=\"1656px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"7.72947%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">A</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Falcon</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"25%\" x=\"75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">9</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.5%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.86473%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.86473%\" x=\"7.72947%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">lifted</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.66184%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.41546%\" x=\"11.5942%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">off</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.8019%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.89855%\" x=\"14.0097%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.4589%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.5266%\" x=\"16.9082%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"25%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Space</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"28.5714%\" x=\"25%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Launch</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"32.1429%\" x=\"53.5714%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Complex</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.6429%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"14.2857%\" x=\"85.7143%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">40</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"92.8571%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.6715%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.93237%\" x=\"30.4348%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.401%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"19.3237%\" x=\"32.3671%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"15%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Cape</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"27.5%\" x=\"15%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Canaveral</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.75%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"17.5%\" x=\"42.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Space</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"17.5%\" x=\"60%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Force</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.75%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"22.5%\" x=\"77.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Station</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.029%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.93237%\" x=\"51.6908%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.657%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.34783%\" x=\"53.6232%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Florida</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.7971%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.93237%\" x=\"57.971%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.9372%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"10.1449%\" x=\"59.9034%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"28.5714%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">9:44</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"28.5714%\" x=\"28.5714%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a.m.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"42.8571%\" x=\"57.1429%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Eastern</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.5714%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.9758%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.38164%\" x=\"70.0483%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">after</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.7391%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.66184%\" x=\"73.43%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"20%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"45%\" x=\"20%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">one-day</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.5%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35%\" x=\"65%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">delay</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.5%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.2609%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.34783%\" x=\"83.0918%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">because</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.2657%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.93237%\" x=\"87.4396%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.4058%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.17874%\" x=\"89.372%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"52.6316%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">recovery</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.3158%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.3684%\" x=\"52.6316%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">weather</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.3158%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.9614%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.44928%\" x=\"98.5507%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.2754%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NP', [('A', 'DT'), ('Falcon', 'NNP'), ('9', 'CD')]), ('lifted', 'VBD'), ('off', 'IN'), ('from', 'IN'), Tree('NP', [('Space', 'NNP'), ('Launch', 'NNP'), ('Complex', 'NNP'), ('40', 'CD')]), ('at', 'IN'), Tree('NP', [('Cape', 'NNP'), ('Canaveral', 'NNP'), ('Space', 'NNP'), ('Force', 'NNP'), ('Station', 'NNP')]), ('in', 'IN'), Tree('NP', [('Florida', 'NNP')]), ('at', 'IN'), Tree('NP', [('9:44', 'CD'), ('a.m.', 'NN'), ('Eastern', 'NNP')]), ('after', 'IN'), Tree('NP', [('a', 'DT'), ('one-day', 'JJ'), ('delay', 'NN')]), ('because', 'IN'), ('of', 'IN'), Tree('NP', [('recovery', 'NN'), ('weather', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_pos_tags_from_file(filename):\n",
    "    file = open(\"data/\"+filename, \"r\", encoding='utf-8')\n",
    "    \n",
    "    sents = nltk.tokenize.sent_tokenize(file.read())\n",
    "    file.close()\n",
    "    \n",
    "    tokenized = []\n",
    "    for sent in sents:\n",
    "        tokenized.append(nltk.tokenize.word_tokenize(sent))\n",
    "        \n",
    "    pos_tags = []\n",
    "    for sent in tokenized:\n",
    "        pos_tags.append(nltk.pos_tag(sent))\n",
    "    \n",
    "    return pos_tags\n",
    "    \n",
    "starlink_tagged = get_pos_tags_from_file(\"starlink.txt\")\n",
    "\n",
    "#Testing\n",
    "starlink_chunked = chunk_parser.parse(starlink_tagged[0])\n",
    "starlink_chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Tree('S', [('A', 'DT'), Tree('NP1', [('Falcon', 'NNP')]), ('9', 'CD'), ('lifted', 'VBD'), ('off', 'IN'), ('from', 'IN'), Tree('NP1', [('Space', 'NNP')]), Tree('NP1', [('Launch', 'NNP')]), Tree('NP1', [('Complex', 'NNP')]), ('40', 'CD'), ('at', 'IN'), Tree('NP1', [('Cape', 'NNP')]), Tree('NP1', [('Canaveral', 'NNP')]), Tree('NP1', [('Space', 'NNP')]), Tree('NP1', [('Force', 'NNP')]), Tree('NP1', [('Station', 'NNP')]), ('in', 'IN'), Tree('NP1', [('Florida', 'NNP')]), ('at', 'IN'), ('9:44', 'CD'), ('a.m.', 'NN'), Tree('NP1', [('Eastern', 'NNP')]), ('after', 'IN'), Tree('NP2', [('a', 'DT'), ('one-day', 'JJ'), ('delay', 'NN')]), ('because', 'IN'), ('of', 'IN'), Tree('NP3', [('recovery', 'NN'), ('weather', 'NN')]), ('.', '.')])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c)\n",
    "From all found subtrees in the text, print out the text from all the leaves on the form of `DT -> JJ -> NN` (i.e. the CONSECUTIVE chunk you defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_descriptive_nouns(tagged_sents):\n",
    "    tuples = set()\n",
    "    \n",
    "    for sent in tagged_sents:\n",
    "        tree = chunk_parser.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"NP3\":\n",
    "                tuples.add(\" \".join([w for w, POS in subtree.leaves()]))\n",
    "    return list(tuples)\n",
    "\n",
    "get_descriptive_nouns(starlink_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```['ground station',\n",
    " 'cargo mission',\n",
    " 'rideshare missions',\n",
    " 'propulsion systems',\n",
    " 'launch webcast',\n",
    " 'rideshare mission',\n",
    " 'crew mission',\n",
    " 'recovery weather']```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d)\n",
    "Create a custom rule for a combination of 3 or more tags, similarly to task c).\n",
    "\n",
    "Do you see any practical uses for chunking with this rule, or in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Context-free grammar (CFG)\n",
    "\n",
    "## 3a)\n",
    "Create a cfg to handle sentences such as \"she is programming\", \"they are coding\" (look at the word forms and POS). The first verb should only handle present tense, while the second verb is flexible. Note that you need to specify the accepted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VBP^<TOP> -> VB NP^<VP-TOP>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = nltk.CFG.fromstring(\"VBP^<TOP> -> VB NP^<VP-TOP>\")\n",
    "cfg.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[VBP^<TOP> -> VB NP^<VP-TOP>]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little function to visualize some possible outputs of the grammar\n",
    "#### Do not change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VB']\n",
      "['VB']\n",
      "['VB']\n",
      "['VB']\n",
      "['VB']\n",
      "['VB']\n",
      "['VB']\n",
      "['VB']\n",
      "['VB']\n",
      "['VB']\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(grammar, start, tokens):        \n",
    "    # iterate left hand and right hand side of the tree\n",
    "    if start in grammar._lhs_index:\n",
    "        derivation = random.choice(grammar._lhs_index[start])            \n",
    "        for rhs in derivation._rhs:          \n",
    "            generate_sample(grammar, rhs, tokens)\n",
    "    elif start in grammar._rhs_index:\n",
    "        tokens.append(str(start))\n",
    "    return tokens\n",
    "\n",
    "for _ in range(10):\n",
    "    print(generate_sample(cfg, cfg.start(), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b)\n",
    "Find some problems with the above CFG, any ideas how you would improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3c)\n",
    "Initialize a `ChartParser` with the cfg from 3a).\n",
    "\n",
    "Write a function to retrieve words not defined by your grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_parser = None # TODO chartparser of cfg grammar above\n",
    "\n",
    "\"\"\"\n",
    "TODO\n",
    "write a function that returns a list of missing tokens (not covered by your cfg)\n",
    "Look up \"lexical index\" of a grammar.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(parser, cfg_grammar, sent):\n",
    "    tokens = None # TODO word tokens\n",
    "    missing = get_missing_words(cfg_grammar, tokens)\n",
    "    if missing:\n",
    "        print(\"Grammar does not cover: {}\".format(missing))\n",
    "        return\n",
    "    trees = None # TODO: a list of parsed tokens\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "    if len(trees) > 0:\n",
    "        return trees[0]\n",
    "    else:\n",
    "        print(\"Ungrammatical sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d)\n",
    "output the tree of your parser for the sentence \"they are programming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"they are programming\"\n",
    "parse(cfg_parser, cfg, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Tweet like Trump! Now that he's banned\n",
    "Using the provided file \"realDonaldTrump.json\", you will build a language model to generate Trump-esque tweets using n-grams.\n",
    "\n",
    "Hint: make use of \"padded_everygram_pipeline\" supported in nltk.lm. This creates all ngrams up to the specified N-parameter with padding:\n",
    "\n",
    "Example:\n",
    "```\n",
    "('<s>',)\n",
    "('<s>', '<s>')\n",
    "('<s>', '<s>', '<s>')\n",
    "('<s>', '<s>', '<s>', '<s>')\n",
    "('<s>', '<s>', '<s>', '<s>', 'i')\n",
    "('<s>',)\n",
    "('<s>', '<s>')\n",
    "('<s>', '<s>', '<s>')\n",
    "('<s>', '<s>', '<s>', 'i')\n",
    "('<s>', '<s>', '<s>', 'i', 'am')\n",
    "('<s>',)\n",
    "('<s>', '<s>')\n",
    "('<s>', '<s>', 'i')\n",
    "('<s>', '<s>', 'i', 'am')\n",
    "('<s>', '<s>', 'i', 'am', 'asking')\n",
    "('<s>',)\n",
    "('<s>', 'i')\n",
    "('<s>', 'i', 'am')\n",
    "('<s>', 'i', 'am', 'asking')\n",
    "('<s>', 'i', 'am', 'asking', 'for')\n",
    "('i',)\n",
    "('i', 'am')\n",
    "('i', 'am', 'asking')\n",
    "('i', 'am', 'asking', 'for')\n",
    "('i', 'am', 'asking', 'for', 'everyone')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "# TODO imports for nltk n-gram modeling and LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the JSON data and store the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/realDonaldTrump.json\") as fp:\n",
    "    tweets = json.load(fp)\n",
    "\n",
    "texts = list(map(lambda x: x.get(\"text\"), tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use this path to store our model after training, so it's easier to reuse later :)\n",
    "pickle_path = \"model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish the `train_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data):\n",
    "    tokenized = None  # TODO: use a tokenizer for the twitter data\n",
    "    n = 0 # TODO find an appropriate N-gram \n",
    "    train_data, padded_vocab = None # TODO: padded everygram\n",
    "    # \n",
    "    model = None # TODO: from nltk.lm (language model), use an appropriate estimator\n",
    "    model.fit(train_data, padded_vocab)\n",
    "    # save the model if you want to :-) then we can load it in the next step without retraining!\n",
    "    with open(pickle_path, \"wb\") as fp:\n",
    "        pickle.dump(model, fp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(pickle_path):\n",
    "    with open(pickle_path, \"rb\") as fp:\n",
    "        model = pickle.load(fp)\n",
    "else:\n",
    "    model = train_model(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, txt):\n",
    "    txt = None  # TODO: tokenize the input\n",
    "    while True:\n",
    "        next_word = model.generate(text_seed=txt, random_seed=42)\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        txt.append(next_word)\n",
    "        \n",
    "    def filter_fn(txt):\n",
    "        no_http = \"http\" not in txt\n",
    "        some_other_rule = True\n",
    "        \n",
    "        return no_http or some_other_rule\n",
    "    \n",
    "    return \" \".join([t for t in txt if filter_fn(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence(model, \"some sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b)\n",
    "Create a grammar to chunk some typical trump statements.\n",
    "\n",
    "There are multiple approaches to this. One way would be to use your own input to the model and look at the resulting outputs and their POS tags. Another possible approach is to use the training data to group together e.g. 5-grams of POS tags to look at the most frequently occurring POS tag groupings. The aim is to have a chunker that groups utterances like \"so sad\", \"make america great again!\" and so forth.\n",
    "\n",
    "Show your results using the outputs from your model with these inputs: \n",
    "- \"clinton will\"\n",
    "- \"obama is\"\n",
    "- \"build a\"\n",
    "- \"so sad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_grammar = r\"\"\"\n",
    "TODO\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
